# -*- coding: utf-8 -*-
"""LLM_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ik6Sb_bOSuX0KH0fams85v0snbApdjTK
"""

OPENAI_API_KEY = ""
TAVILY_API_KEY = ""

USE_TEST_DATA = False
USE_MODEL = "gpt-5-mini"
LOAD_QWEN = True
FIRST_RUN = True
DEBUG_MODE = False

if FIRST_RUN:
    import subprocess
    print("Installing dependencies...")
    subprocess.run(["pip", "install", "-q", "-U",
        "langchain", "langchain-community", "langchain-huggingface", "langchain-openai",
        "transformers", "accelerate", "bitsandbytes", "sentencepiece",
        "tavily-python", "datasets", "pandas", "matplotlib", "seaborn"])
    print("Dependencies installed!")

import warnings
warnings.filterwarnings("ignore")
import os
import json
import time
import re
import pandas as pd
import matplotlib.pyplot as plt
from typing import Literal
from dataclasses import dataclass

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY

assert OPENAI_API_KEY, "Please enter your OpenAI API key"
assert TAVILY_API_KEY, "Please enter your Tavily API key"
print("API Keys configured!")

# ============ Load Search Tool (Tavily) ============
from tavily import TavilyClient

tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

def web_search(query: str, max_results: int = 3) -> str:
    try:
        response = tavily_client.search(query=query, max_results=max_results)
        results = response.get("results", [])
        if not results:
            return "No search results found."
        formatted = "\n".join([f"- {r['title']}: {r['content'][:300]}..." for r in results])
        return formatted
    except Exception as e:
        return f"Search error: {str(e)}"

print("Testing Tavily Search...")
test_result = web_search("2024 US Presidential Election winner")
print(test_result[:150] + "...\n")

# ============ Load Models ============
from langchain_openai import ChatOpenAI

gpt_llm = ChatOpenAI(model=USE_MODEL, temperature=0.1, max_tokens=512)
print(f"{USE_MODEL} loaded!")

qwen_llm = None
if LOAD_QWEN:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
    from langchain_huggingface import HuggingFacePipeline

    model_id = "Qwen/Qwen2.5-7B-Instruct"
    print(f"Loading {model_id}...")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    qwen_model = AutoModelForCausalLM.from_pretrained(
        model_id, quantization_config=bnb_config, device_map="auto", trust_remote_code=True
    )
    pipe = pipeline("text-generation", model=qwen_model, tokenizer=tokenizer,
                    max_new_tokens=512, temperature=0.1, do_sample=True, return_full_text=False)
    qwen_llm = HuggingFacePipeline(pipeline=pipe)
    print("Qwen 7B loaded!")

# ============ Load Datasets ============
def load_freshqa_hf():
    """Load FreshQA-multilingual from HuggingFace."""
    print("Loading FreshQA from HuggingFace...")
    try:
        from datasets import load_dataset
        ds = load_dataset("SeaLLMs/FreshQA-multilingual", split="test")

        fresh_questions = []
        for item in ds:
            question = item.get("question", "")
            answer = item.get("answer", "")

            if question and answer:
                fresh_questions.append({
                    "type": "Dynamic",
                    "question": question,
                    "answer": answer,
                    "aliases": [],
                    "source": "FreshQA"
                })

        print(f"  Loaded {len(fresh_questions)} FreshQA questions")
        return fresh_questions
    except Exception as e:
        print(f"  FreshQA load failed: {e}")
        return None

def load_sealqa():
    """Load SealQA (improved FreshQA) from HuggingFace."""
    print("Loading SealQA from HuggingFace...")
    try:
        from datasets import load_dataset
        ds = load_dataset("vtllms/sealqa", split="test")

        questions = []
        for item in ds:
            question = item.get("question", "")
            answer = item.get("answer", "")

            if question and answer:
                questions.append({
                    "type": "Dynamic",
                    "question": question,
                    "answer": answer,
                    "aliases": [],
                    "source": "SealQA"
                })

        print(f"  Loaded {len(questions)} SealQA questions")
        return questions
    except Exception as e:
        print(f"  SealQA load failed: {e}")
        return None

if USE_TEST_DATA:
    print("\nTEST MODE: Using minimal dataset")

    static_questions = [
        {"type": "Static", "question": "What is the capital of France?",
         "answer": "Paris", "aliases": ["paris"]},
        {"type": "Static", "question": "Who wrote Romeo and Juliet?",
         "answer": "William Shakespeare", "aliases": ["shakespeare"]},
        {"type": "Static", "question": "Who won the 2024 Super Bowl in February 2024?",
         "answer": "Kansas City Chiefs", "aliases": ["chiefs", "kc chiefs", "kansas city"]},
    ]

    dynamic_questions = [
        {"type": "Dynamic", "question": "Who won the 2024 US Presidential Election in November 2024?",
         "answer": "Donald Trump", "aliases": ["trump", "donald j. trump"]},
        {"type": "Dynamic", "question": "What is the current stock price of Tesla today?",
         "answer": None, "aliases": []},
        {"type": "Dynamic", "question": "When was GPT-5 released by OpenAI in 2025?",
         "answer": "August 2025", "aliases": ["august 7, 2025", "august 7 2025", "august 2025"]},
        {"type": "Dynamic", "question": "Who won the 2024 NBA Finals in June 2024?",
         "answer": "Boston Celtics", "aliases": ["celtics", "boston"]},
        {"type": "Dynamic", "question": "What AI model with computer use capability did Anthropic release in October 2024?",
         "answer": "Claude 3.5 Sonnet", "aliases": ["claude 3.5", "3.5 sonnet"]},
    ]
else:
    from datasets import load_dataset

    # ========== STATIC: TriviaQA ==========
    print("\nLoading TriviaQA (Static Knowledge)...")
    trivia_ds = load_dataset("trivia_qa", "rc", split="validation")
    trivia_sample = trivia_ds.shuffle(seed=42).select(range(50))
    static_questions = [
        {"type": "Static", "question": item["question"],
         "answer": item["answer"]["value"],
         "aliases": [a.lower() for a in item["answer"]["aliases"]]}
        for item in trivia_sample
    ]
    print(f"  Loaded {len(static_questions)} TriviaQA questions")

    # ========== DYNAMIC: FreshQA / SealQA + Custom ==========
    freshqa_data = load_freshqa_hf()
    sealqa_data = load_sealqa()

    # Custom questions for events after May 2024 (model's knowledge cutoff)
    custom_dynamic = [
        {"type": "Dynamic", "question": "Who won the 2024 US Presidential Election in November 2024?",
         "answer": "Donald Trump", "aliases": ["trump", "donald j. trump"]},
        {"type": "Dynamic", "question": "What is the current price of Bitcoin today?",
         "answer": None, "aliases": []},
        {"type": "Dynamic", "question": "When was GPT-5 released by OpenAI?",
         "answer": "August 2025", "aliases": ["august 7, 2025", "august 7 2025"]},
        {"type": "Dynamic", "question": "Who won the 2024 NBA Finals in June 2024?",
         "answer": "Boston Celtics", "aliases": ["celtics", "boston"]},
        {"type": "Dynamic", "question": "What AI model with computer use capability did Anthropic release in October 2024?",
         "answer": "Claude 3.5 Sonnet", "aliases": ["claude 3.5", "3.5 sonnet"]},
        {"type": "Dynamic", "question": "Who won the 2024 UEFA European Championship (Euro 2024)?",
         "answer": "Spain", "aliases": ["spain national team"]},
        {"type": "Dynamic", "question": "What was the name of the major hurricane that hit Florida in October 2024?",
         "answer": "Hurricane Milton", "aliases": ["milton"]},
        {"type": "Dynamic", "question": "Who became the new Prime Minister of Japan in October 2024?",
         "answer": "Shigeru Ishiba", "aliases": ["ishiba"]},
        {"type": "Dynamic", "question": "Who won the 2024 Ballon d'Or award?",
         "answer": "Rodri", "aliases": ["rodrigo hernandez"]},
        {"type": "Dynamic", "question": "Who won the 2024 Nobel Prize in Physics for AI research?",
         "answer": "Geoffrey Hinton", "aliases": ["hinton", "john hopfield"]},
    ]

    # Combine
    dynamic_questions = []

    if freshqa_data:
        dynamic_questions.extend(freshqa_data[:15])

    if sealqa_data:
        dynamic_questions.extend(sealqa_data[:10])

    dynamic_questions.extend(custom_dynamic)

    seen = set()
    unique_dynamic = []
    for q in dynamic_questions:
        q_text = q["question"].lower()[:50]
        if q_text not in seen:
            seen.add(q_text)
            unique_dynamic.append(q)
    dynamic_questions = unique_dynamic[:25]
    print(f"  Total dynamic questions: {len(dynamic_questions)}")

print(f"  Static: {len(static_questions)} | Dynamic: {len(dynamic_questions)}")
all_questions = static_questions + dynamic_questions
print(f"Total: {len(all_questions)} questions\n")

# ============ Answer Evaluation ============
def check_answer_correct(response: str, ground_truth: str, aliases: list) -> bool:
    """Stricter answer checking - requires meaningful match."""
    if ground_truth is None:
        if re.search(r'\d+', response):
            return True
        return False

    if not response or response.strip() == "":
        return False

    response_lower = response.lower().strip()
    ground_truth_lower = ground_truth.lower().strip()

    if ground_truth_lower in response_lower:
        return True

    for alias in aliases:
        alias_lower = alias.lower().strip()
        if len(alias_lower) >= 4 and alias_lower in response_lower:
            return True

    response_clean = re.sub(r'[^\w\s]', '', response_lower)
    truth_clean = re.sub(r'[^\w\s]', '', ground_truth_lower)

    if truth_clean in response_clean:
        return True

    return False

# ============ Agent Logic ============
@dataclass
class AgentResult:
    response: str
    tool_used: bool
    latency: float
    reasoning: str

def smart_agent(query: str, llm, is_qwen: bool = False) -> AgentResult:
    start = time.time()

    assess_prompt = f"""Your knowledge cutoff is May 31, 2024. You have no information about events after this date.

Question: {query}

Do you need to search the web to answer this question accurately?
- Answer YES if: the question requires information after your cutoff date, or real-time data you cannot know
- Answer NO if: you can answer accurately from your existing knowledge

Reply with ONLY: YES or NO"""

    needs_search = False
    reasoning = "unknown"

    try:
        if is_qwen:
            raw = llm.invoke(assess_prompt)
            if hasattr(raw, 'content'):
                raw = raw.content
            raw = str(raw).strip()
        else:
            response = llm.invoke(assess_prompt)
            raw = response.content if hasattr(response, 'content') else str(response)

        raw_clean = raw.strip().upper()
        if DEBUG_MODE:
            print(f"      [DEBUG] Assessment: '{raw.strip()[:50]}'")

        # Parse YES/NO
        first_word = raw_clean.split()[0] if raw_clean.split() else ""
        if "YES" in first_word or raw_clean.startswith("YES"):
            needs_search = True
            reasoning = "model decided: needs search"
        elif "NO" in first_word or raw_clean.startswith("NO"):
            needs_search = False
            reasoning = "model decided: internal knowledge"
        else:
            raise ValueError(f"Could not parse: {raw[:50]}")
    except Exception as e:
        if DEBUG_MODE:
            print(f"      [DEBUG] Parse failed: {e}")
        query_lower = query.lower()
        if any(kw in query_lower for kw in ["today", "current", "now", "latest", "2025", "2024"]):
            needs_search = True
            reasoning = "fallback: time-sensitive keywords"
        else:
            needs_search = False
            reasoning = "fallback: general knowledge"

    # Generate answer
    if needs_search:
        search_results = web_search(query)
        if DEBUG_MODE:
            print(f"      [DEBUG] Search: {search_results[:100]}...")
        answer_prompt = f"Question: {query}\n\nSearch Results:\n{search_results}\n\nProvide a direct answer based on the search results."
    else:
        answer_prompt = f"Question: {query}\n\nAnswer directly and concisely."

    try:
        if is_qwen:
            ans = llm.invoke(answer_prompt)
            final = str(ans).strip() if not hasattr(ans, 'content') else ans.content
        else:
            ans = llm.invoke(answer_prompt)
            final = ans.content if hasattr(ans, 'content') else str(ans)
    except Exception as e:
        final = f"Error: {e}"

    if DEBUG_MODE:
        print(f"      [DEBUG] Answer: '{final.strip()[:60]}'")

    return AgentResult(response=final.strip(), tool_used=needs_search,
                       latency=time.time() - start, reasoning=reasoning)

def naive_agent(query: str, llm, is_qwen: bool = False) -> AgentResult:
    start = time.time()
    search_results = web_search(query)

    answer_prompt = f"Question: {query}\n\nSearch Results:\n{search_results}\n\nProvide a direct answer based on the search results."

    try:
        if is_qwen:
            ans = llm.invoke(answer_prompt)
            final = str(ans).strip() if not hasattr(ans, 'content') else ans.content
        else:
            ans = llm.invoke(answer_prompt)
            final = ans.content if hasattr(ans, 'content') else str(ans)
    except Exception as e:
        final = f"Error: {e}"

    return AgentResult(response=final.strip(), tool_used=True,
                       latency=time.time() - start, reasoning="always_search")

# ============ Evaluation ============
def calculate_metrics(results: list[dict]) -> dict:
    static = [r for r in results if r["type"] == "Static"]
    dynamic = [r for r in results if r["type"] == "Dynamic"]

    static_correct_dec = sum(1 for r in static if not r["tool_used"])
    dynamic_correct_dec = sum(1 for r in dynamic if r["tool_used"])

    return {
        "static_accuracy": sum(r["correct"] for r in static) / len(static) if static else 0,
        "dynamic_accuracy": sum(r["correct"] for r in dynamic) / len(dynamic) if dynamic else 0,
        "static_tool_rate": sum(r["tool_used"] for r in static) / len(static) if static else 0,
        "dynamic_tool_rate": sum(r["tool_used"] for r in dynamic) / len(dynamic) if dynamic else 0,
        "static_decision_acc": static_correct_dec / len(static) if static else 0,
        "dynamic_decision_acc": dynamic_correct_dec / len(dynamic) if dynamic else 0,
        "overall_decision_acc": (static_correct_dec + dynamic_correct_dec) / len(results) if results else 0,
        "avg_latency": sum(r["latency"] for r in results) / len(results) if results else 0,
        "hallucination_rate": sum(1 for r in dynamic if not r["tool_used"] and not r["correct"]) / len(dynamic) if dynamic else 0,
    }

def run_evaluation(questions, llm, agent_type, model_name, is_qwen=False):
    results = []
    print(f"\nRunning {agent_type.upper()} agent with {model_name}...")

    for i, q in enumerate(questions):
        print(f"  [{i+1}/{len(questions)}] {q['question'][:45]}...")

        if agent_type == "smart":
            result = smart_agent(q["question"], llm, is_qwen)
        else:
            result = naive_agent(q["question"], llm, is_qwen)

        correct = check_answer_correct(result.response, q["answer"], q.get("aliases", []))

        results.append({
            "type": q["type"], "question": q["question"], "ground_truth": q["answer"],
            "response": result.response, "tool_used": result.tool_used,
            "latency": result.latency, "reasoning": result.reasoning,
            "correct": correct, "model": model_name, "agent": agent_type
        })
        time.sleep(0.3)
    return results

# ============ Run All Experiments ============
all_results = {}

# 1. GPT-5-mini experiments
print("="*60)
print(f"EXPERIMENT 1: {USE_MODEL}")
print("="*60)
all_results[f"{USE_MODEL}_smart"] = run_evaluation(all_questions, gpt_llm, "smart", USE_MODEL)
all_results[f"{USE_MODEL}_naive"] = run_evaluation(all_questions, gpt_llm, "naive", USE_MODEL)

# 2. Qwen experiments
if LOAD_QWEN and qwen_llm:
    print("\n" + "="*60)
    print("EXPERIMENT 2: Qwen 7B")
    print("="*60)
    all_results["Qwen-7B_smart"] = run_evaluation(all_questions, qwen_llm, "smart", "Qwen-7B", is_qwen=True)
    all_results["Qwen-7B_naive"] = run_evaluation(all_questions, qwen_llm, "naive", "Qwen-7B", is_qwen=True)

# ============ Calculate All Metrics ============
metrics = {name: calculate_metrics(results) for name, results in all_results.items()}

# ============ Results Comparison ============
print("\n" + "="*60)
print("EVALUATION RESULTS - MODEL COMPARISON")
print("="*60)

# Build comparison table
metric_names = ["Static Answer Acc", "Dynamic Answer Acc", "Static Decision Acc",
                "Dynamic Decision Acc", "Overall Decision Acc", "Static Tool Rate",
                "Dynamic Tool Rate", "Avg Latency (s)", "Hallucination Rate"]

metric_keys = ["static_accuracy", "dynamic_accuracy", "static_decision_acc",
               "dynamic_decision_acc", "overall_decision_acc", "static_tool_rate",
               "dynamic_tool_rate", "avg_latency", "hallucination_rate"]

comparison_data = {"Metric": metric_names}

for name, m in metrics.items():
    col_data = []
    for key in metric_keys:
        if "naive" in name and "decision" in key:
            col_data.append("N/A")
        elif key == "avg_latency":
            col_data.append(f"{m[key]:.2f}")
        else:
            col_data.append(f"{m[key]:.1%}")
    comparison_data[name] = col_data

df_comparison = pd.DataFrame(comparison_data)
print(df_comparison.to_string(index=False))

# ============ Visualization ============
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

smart_keys = [k for k in metrics.keys() if "smart" in k]
colors = ["#3498db", "#e74c3c", "#2ecc71", "#9b59b6"]

# 1. Decision Accuracy Comparison
ax1 = axes[0, 0]
x = range(len(smart_keys))
width = 0.25
for i, key in enumerate(smart_keys):
    m = metrics[key]
    vals = [m["static_decision_acc"], m["dynamic_decision_acc"], m["overall_decision_acc"]]
    positions = [j + i*width for j in range(3)]
    ax1.bar(positions, vals, width, label=key.replace("_smart", ""), color=colors[i])
ax1.set_ylabel("Accuracy")
ax1.set_title("Decision Accuracy by Model")
ax1.set_xticks([j + width/2 for j in range(3)])
ax1.set_xticklabels(["Static\n(should NOT search)", "Dynamic\n(should search)", "Overall"])
ax1.legend()
ax1.set_ylim(0, 1.1)

# 2. Tool Usage Rate
ax2 = axes[0, 1]
for i, key in enumerate(smart_keys):
    m = metrics[key]
    vals = [m["static_tool_rate"], m["dynamic_tool_rate"]]
    positions = [j + i*width for j in range(2)]
    ax2.bar(positions, vals, width, label=key.replace("_smart", ""), color=colors[i])
ax2.set_ylabel("Tool Usage Rate")
ax2.set_title("Tool Usage by Model (Smart Agent)")
ax2.set_xticks([j + width/2 for j in range(2)])
ax2.set_xticklabels(["Static Questions", "Dynamic Questions"])
ax2.legend()
ax2.set_ylim(0, 1.1)

# 3. Latency Comparison
ax3 = axes[1, 0]
model_names = []
latencies_smart = []
latencies_naive = []
for key in smart_keys:
    model_name = key.replace("_smart", "")
    model_names.append(model_name)
    latencies_smart.append(metrics[key]["avg_latency"])
    naive_key = key.replace("_smart", "_naive")
    if naive_key in metrics:
        latencies_naive.append(metrics[naive_key]["avg_latency"])
    else:
        latencies_naive.append(0)

x = range(len(model_names))
ax3.bar([i - width/2 for i in x], latencies_smart, width, label="Smart", color="#3498db")
ax3.bar([i + width/2 for i in x], latencies_naive, width, label="Naive", color="#e74c3c")
ax3.set_ylabel("Latency (seconds)")
ax3.set_title("⏱️ Average Latency Comparison")
ax3.set_xticks(x)
ax3.set_xticklabels(model_names)
ax3.legend()

# 4. Answer Accuracy
ax4 = axes[1, 1]
for i, key in enumerate(smart_keys):
    m = metrics[key]
    vals = [m["static_accuracy"], m["dynamic_accuracy"]]
    positions = [j + i*width for j in range(2)]
    ax4.bar(positions, vals, width, label=key.replace("_smart", ""), color=colors[i])
ax4.set_ylabel("Accuracy")
ax4.set_title("Answer Accuracy by Model")
ax4.set_xticks([j + width/2 for j in range(2)])
ax4.set_xticklabels(["Static Questions", "Dynamic Questions"])
ax4.legend()
ax4.set_ylim(0, 1.1)

plt.tight_layout()
plt.savefig("model_comparison_results.png", dpi=150)
plt.show()

# ============ Qualitative Analysis ============
print("\n" + "="*60)
print("QUALITATIVE ANALYSIS - Sample Decisions")
print("="*60)

for model_key in smart_keys:
    print(f"\n--- {model_key} ---")
    for r in all_results[model_key][:5]:
        decision = "SEARCHED" if r["tool_used"] else "INTERNAL"
        expected = "should search" if r["type"] == "Dynamic" else "should NOT search"
        correct_dec = (r["type"] == "Dynamic" and r["tool_used"]) or (r["type"] == "Static" and not r["tool_used"])

        print(f"\n  [{r['type']}] {r['question'][:50]}...")
        print(f"      Decision: {decision} ({expected}) {'✓' if correct_dec else '✗'}")
        print(f"      Answer: {r['response'][:80] if r['response'] else '(empty)'}...")
        print(f"      Correct: {'✓' if r['correct'] else '✗'}")

# ============ Summary ============
print("\n" + "="*60)
print("KEY FINDINGS - MODEL COMPARISON")
print("="*60)

for model_key in smart_keys:
    m = metrics[model_key]
    naive_key = model_key.replace("_smart", "_naive")
    m_naive = metrics.get(naive_key, {})

    print(f"\n{model_key.replace('_smart', '')}:")
    print(f"   Decision Accuracy: {m['overall_decision_acc']:.1%}")
    print(f"   Answer Accuracy: Static {m['static_accuracy']:.1%}, Dynamic {m['dynamic_accuracy']:.1%}")
    print(f"   Latency: Smart {m['avg_latency']:.2f}s vs Naive {m_naive.get('avg_latency', 0):.2f}s")
    print(f"   Unnecessary Searches Avoided: {1 - m['static_tool_rate']:.1%}")
    print(f"   Hallucination Rate: {m['hallucination_rate']:.1%}")

# ============ Save Results ============
all_results_flat = []
for name, results in all_results.items():
    all_results_flat.extend(results)

results_df = pd.DataFrame(all_results_flat)
results_df.to_csv("full_evaluation_results.csv", index=False)
print("\nResults saved to full_evaluation_results.csv")
print("Charts saved to model_comparison_results.png")